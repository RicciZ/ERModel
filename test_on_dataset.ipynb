{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import DREAMER\n",
    "from ERModel import ERModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(filename, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(object):\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.tp, self.tn, self.fp, self.fn = 0, 0, 0, 0\n",
    "    \n",
    "    def update(self, y, y_):\n",
    "        y = (y >= 3).long()\n",
    "        y_ = (y_ >= 3).long()\n",
    "        self.tp += sum(y_[y == 1] == 1)\n",
    "        self.tn += sum(y_[y == 0] == 0)\n",
    "        self.fp += sum(y_[y == 0] == 1)\n",
    "        self.fn += sum(y_[y == 1] == 0)\n",
    "        self.count += len(y)\n",
    "    \n",
    "    def printinfo(self):\n",
    "        print(f\"tp={self.tp}, tn={self.tn}, fp={self.fp}, fn={self.fn}, count={self.count}\")\n",
    "\n",
    "    def f1(self):\n",
    "        precision = self.tp/(self.tp + self.fp + 0.001)\n",
    "        recall = self.tp/(self.tp + self.fn + 0.001)\n",
    "        return 2*precision*recall / (precision + recall + 0.001)\n",
    "    \n",
    "    def acc(self):\n",
    "        return self.tp/self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model, train=True):\n",
    "    if train:\n",
    "        print(\"Checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on testing data\")\n",
    "\n",
    "    metric_val = Metric()\n",
    "    metric_aro = Metric()\n",
    "    metric_dom = Metric()\n",
    "    mse_sum = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # set model to evaluation mode\n",
    "\n",
    "    # don't compute the gradient in this step\n",
    "    with torch.no_grad(): \n",
    "        for x, y in loader:\n",
    "            x = x.permute(0,2,1).float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            y_hat = model(x)\n",
    "            metric_val.update(y[:,0], y_hat[:,0])\n",
    "            metric_aro.update(y[:,1], y_hat[:,1])\n",
    "            metric_dom.update(y[:,2], y_hat[:,2])\n",
    "            mse_sum += torch.sum(torch.mean((y_hat-y)**2,1))\n",
    "            num_samples += y_hat.shape[0]\n",
    "            # metric_val.printinfo()\n",
    "            # metric_aro.printinfo()\n",
    "            # metric_dom.printinfo()\n",
    "            \n",
    "        print(f\"accuracy: {(metric_val.acc(), metric_aro.acc(), metric_dom.acc())}\")\n",
    "        print(f\"f1 score: {(metric_val.f1(), metric_aro.f1(), metric_dom.f1())}\")\n",
    "        print(f\"Got mean mse {mse_sum/num_samples:.2f}\")\n",
    "\n",
    "    model.train() # set back to train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "num_classes = 3\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "input_size = 60000\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "lr = 0.001\n",
    "use_hrv = True\n",
    "device = torch.device(\"cpu\")\n",
    "model = ERModel(input_size, hidden_size, num_layers, num_classes, device, use_hrv).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "batch_size = 32\n",
    "dataset = DREAMER(input_size, use_hrv)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [380, 34])\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "load_checkpoint(os.path.join('checkpoint/my_checkpoint.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on testing data\n",
      "accuracy: (tensor(0.4706), tensor(0.6471), tensor(0.8235))\n",
      "f1 score: (tensor(0.7800), tensor(0.8143), tensor(0.9175))\n",
      "Got mean mse 1.12\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(test_loader, model, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ERCode]",
   "language": "python",
   "name": "conda-env-ERCode-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
